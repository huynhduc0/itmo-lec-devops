x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  image: hehe
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./spark:/opt/airflow/spark
  environment:
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow

  spark-master:
    container_name: spark-master
    image: apache/spark:latest
    depends_on:
      - postgres
    ports:
      - "4040:8080"
      - "7077:7077"
    command: "/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master"

  spark-worker:
    container_name: spark-worker
    image: apache/spark:latest
    depends_on:
      - spark-master
    ports:
      - "7001:7000"
    command: "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"

  airflow-init:
    <<: *airflow-common
    depends_on:
      - spark-worker
    command: >
      bash -c "
      airflow db init && 
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com && 
      airflow version
      "
  webserver:
    <<: *airflow-common
    depends_on:
      - airflow-init
    ports:
      - "8080:8080"
    restart: always
    command: webserver

  scheduler:
    <<: *airflow-common
    depends_on:
      - airflow-init
    command: scheduler
